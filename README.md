# Firecrawl 文章爬虫

一个基于 Python asyncio 和 Firecrawl 的高效异步文章爬取工具，专门用于批量爬取网站文章内容并保存为 Markdown 格式。

## 功能特性

- 🚀 **异步并发爬取**：使用 asyncio 实现高效并发，显著提升爬取速度
- 🔄 **断点续传**：支持检测已存在文件，避免重复爬取
- 🔁 **失败重试**：内置重试机制，提高爬取成功率
- 📄 **格式转换**：自动将爬取内容转换为 Markdown 格式
- 🏷️ **安全命名**：自动生成安全的文件名
- 📊 **进度监控**：实时显示爬取进度和统计信息
- ⚙️ **并发控制**：可配置并发数量和信号量限制

## 项目结构

```
firecrawl-scraper/
├── README.md                          # 项目说明文档
├── pyproject.toml                     # 项目配置和依赖
├── main.py                            # 简单入口文件
├── scrape_asyncio.py                  # 主要爬虫脚本
├── cbre_data_center_articles.json     # 文章列表数据
└── .python-version                    # Python 版本配置
```

## 依赖要求

- Python >= 3.11
- firecrawl-py >= 4.6.0
- requests >= 2.32.5
- Firecrawl 服务器（本地或云端）

## 安装

### 1. 克隆项目

```bash
git clone <repository-url>
cd firecrawl-scraper
```

### 2. 安装依赖

使用 uv（推荐）：

```bash
uv sync
```

或使用 pip：

```bash
pip install -e .
```

### 3. 配置 Firecrawl

确保 Firecrawl 服务正在运行：

```bash
# 启动本地 Firecrawl 服务
firecrawl serve
```

默认配置使用本地服务：
- URL: `http://localhost:8547`
- API Key: `test`（本地服务可使用任意字符串）

## 使用方法

### 准备文章列表

首先需要准备包含文章信息的 JSON 文件，格式如下：

```json
{
  "articles": [
    {
      "title": "文章标题 1",
      "url": "https://example.com/article-1"
    },
    {
      "title": "文章标题 2",
      "url": "https://example.com/article-2"
    }
  ]
}
```

### 运行爬虫

```bash
python scrape_asyncio.py
```

## 配置参数

在 `scrape_asyncio.py` 中可以调整以下配置：

```python
# 并发配置
MAX_CONCURRENT = 15      # 最大并发数（推荐：2×CPU核心数）
SEMAPHORE_LIMIT = 15     # 信号量限制
RETRY_COUNT = 3          # 失败重试次数
RETRY_DELAY = 2.0        # 重试延迟（秒）

# Firecrawl 配置
FIRECRAWL_URL = "http://localhost:8547"  # Firecrawl 服务地址
```

## 输出文件

爬取结果将保存到脚本所在目录，文件命名规则：
```
{序号:03d}_{安全标题}.md
```

例如：
```
001_文章标题示例一.md
002_文章标题示例二.md
```

每个文件包含：
- 文章标题
- 原始 URL
- 爬取时间
- Markdown 格式的文章内容

## 功能详解

### 断点续传

程序会自动检测已存在的 `.md` 文件，跳过已成功爬取的文章，只处理未完成的任务。

### 进度监控

程序会实时显示：
- 当前进度（已处理/总数）
- 成功/失败统计
- 已用时间和预计总用时
- 当前轮次的统计信息

### 错误处理

- 自动重试失败的请求（最多3次）
- 显示详细错误信息
- 记录失败的文章列表
- 不会因单个失败而中断整个任务

## 输出示例

```
⚙️  配置:
  • 最大并发数: 15
  • 重试次数: 3
  • 输出目录: /path/to/project

输出目录: /path/to/project
Firecrawl URL: http://localhost:8547
最大并发数: 15
======================================================================
总共需要爬取 100 篇文章

检测到已有 20 篇文章，剩余 80 篇待爬取
实际需要爬取: 80 篇文章
======================================================================

开始异步并发爬取 (最大并发: 15)...

[1/80] ✓ 成功 (2.3s): 001_示例文章一.md
[2/80] ✓ 成功 (1.8s): 002_示例文章二.md
...

==============================================================
异步爬取完成！
==============================================================
总文章数: 100
之前已成功: 20
本轮成功: 75
本轮失败: 5
总用时: 125.4秒
平均用时: 1.25秒/篇
最大并发数: 15
输出目录: /path/to/project

✅ 异步爬取完成！
```

## 故障排除

### 1. 连接错误
- 确保 Firecrawl 服务正在运行
- 检查 URL 和端口是否正确
- 验证网络连接

### 2. 权限错误
- 确保有写入输出目录的权限
- 检查磁盘空间是否充足

### 3. 大量失败
- 降低并发数（`MAX_CONCURRENT`）
- 增加重试次数（`RETRY_COUNT`）
- 检查目标网站是否对爬虫有限制

## 许可证

MIT License

## 贡献

欢迎提交 Issue 和 Pull Request！

## 联系方式

如有问题，请创建 Issue 反馈。
